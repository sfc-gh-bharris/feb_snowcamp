{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "rjrm4tizyztikiex2gyb",
   "authorId": "1011012698186",
   "authorName": "BHARRIS",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "779cb381-f71b-4e75-ba3b-0551dc3b05a4",
   "lastEditTime": 1738358531247
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "source": "-- In order to cleanup from all the ingest patterns built, you can drop the USER, ROLE, DATABASE, and WAREHOUSE:\nUSE ROLE INGEST;\nDROP DATABASE INGEST;\n\nUSE ROLE ACCOUNTADMIN;\nDROP USER INGEST;\nDROP WAREHOUSE INGEST;\nDROP ROLE INGEST;\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "To tear down docker, run the following in your shell:\n\n``` docker compose down```\n\nTo delete the conda env, run the following in your shell:\n\n```\nconda deactivate\nconda remove -n sf-ingest-examples --all\n```"
  },
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Conclusion And Resources\nAs you've seen, there are many ways to load data into Snowflake. It is important to understand the benefits and consequenses so you can make the right choice when ingesting data into Snowflake.\n\nWhile some examples only focussed on the Python connector, these patterns are often applicable to our other connectors if your language of choice is not Python. Connectors are available for Python, Java, Node.js, Go, .NET, and PHP. Note that based on the load times, batch size would be worth tuning.\n\nServerless Tasks, Snowpipe, and Streaming are all built on Snowflake's serverless compute which make it much simpler to have efficient utilization of infrastructure. Managing warehouses and keeping them fully loaded is not easy or even possible in many cases.\n\nIf you're using the Kafka connector for Snowflake, put it in Streaming mode. It will either be the same or less credit consumption AND make the data available more quickly.\n\nWhen well-sized batches are not possible, leveraging our Streaming ingest will significantly increase efficiency. We will merge those tiny batches together in Snowflake later in a very efficient workflow while making that data available for query quickly.\n\n**What You Learned**\n* How to Ingest data with Connectors\n* Using Serverless Tasks and Snowpipe to save credit consumption\n* How to Use the Kafka Connectors for Snowflake\n* How Streaming reduces the time to Ingest AND Increases Efficiency\n\n**Related Resources**\n*   [Snowflake Connector for Python](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector)\n*   [Java SDK for the Snowflake Ingest Service](https://github.com/snowflakedb/snowflake-ingest-java)\n*   [Python Snowflake Ingest Service SDK](https://github.com/snowflakedb/snowflake-ingest-python)\n*   [Getting Started with Snowpipe](https://quickstarts.snowflake.com/guide/getting_started_with_snowpipe/index.html)\n*   [Getting Started with Snowpipe Streaming and Amazon MSK](https://quickstarts.snowflake.com/guide/getting_started_with_snowpipe_streaming_aws_msk/index.html)\n*   [Streaming Data Integration with Snowflake](https://quickstarts.snowflake.com/guide/data_engineering_streaming_integration/index.html)"
  }
 ]
}