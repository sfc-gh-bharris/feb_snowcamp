{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "v63oi2ocbzvq5kfiju2s",
   "authorId": "394090024763",
   "authorName": "BHARRIS_DEMO",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "9726f081-4da7-47ce-befc-a73076727a51",
   "lastEditTime": 1738885542783
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b47452-3bd7-447a-bae9-109cf781eaeb",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "### Real time loading with Kafka\n\nThere are many use cases across various industries which require real time data: credit scoring, fraud analysis, or even user facing analytics. Delivering data with low latency is typically accomplished by using a message broker like Apache Kafka. Instead of files, Kafka receives messages from a variety of different \"data producers\", and then sends those messages out to different \"data consumers\". In the context of data loading, Snowflake becomes a data consumer.\n\nFrom the perspective of Snowflake, a Kafka topic produces a stream of rows to be inserted into a Snowflake table. In general, each Kafka message contains one row.\n\nKafka, like many message publish/subscribe platforms, allows a many-to-many relationship between publishers and subscribers. A single application can publish to many topics, and a single application can subscribe to multiple topics. With Snowflake, the typical pattern is that one topic supplies messages (rows) for one Snowflake table.\n\nThe current version of the Kafka connector is limited to loading data into Snowflake. The Kafka connector supports two data loading methods:\n\n* Snowpipe\n* Snowpipe Streaming."
  },
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## From Kafka - in Snowpipe (Batch) mode\n\nThe table for the data to be written to will be automatically created by the connector.\n\nConfigure and install the connector to load data. Run the following in your shell:\n\n```\nexport KAFKA_TOPIC=LIFT_TICKETS_KAFKA_BATCH\neval $(cat .env)\n\nURL=\"https://$SNOWFLAKE_ACCOUNT.snowflakecomputing.com\"\nNAME=\"LIFT_TICKETS_KAFKA_BATCH\"\n\ncurl -i -X PUT -H \"Content-Type:application/json\" \\\n    \"http://localhost:8083/connectors/$NAME/config\" \\\n    -d '{\n        \"connector.class\":\"com.snowflake.kafka.connector.SnowflakeSinkConnector\",\n        \"errors.log.enable\":\"true\",\n        \"snowflake.database.name\":\"INGEST\",\n        \"snowflake.private.key\":\"'$PRIVATE_KEY'\",\n        \"snowflake.schema.name\":\"INGEST\",\n        \"snowflake.role.name\":\"INGEST\",\n        \"snowflake.url.name\":\"'$URL'\",\n        \"snowflake.user.name\":\"'$SNOWFLAKE_USER'\",\n        \"topics\":\"'$KAFKA_TOPIC'\",\n        \"name\":\"'$NAME'\",\n        \"buffer.size.bytes\":\"250000000\",\n        \"buffer.flush.time\":\"60\",\n        \"buffer.count.records\":\"1000000\",\n        \"snowflake.topic2table.map\":\"'$KAFKA_TOPIC:$NAME'\"\n    }'\n```\n\nVerify the connector was created and is running in the [Redpanda console](http://localhost:8080/topics).\n"
  },
  {
   "cell_type": "markdown",
   "id": "2916bfa4-983f-4d9f-896f-938a8fb6ee26",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "\nTo start, lets push in one message to get the table created and verify the connector is working.\n\nRun the following in your shell:\n\n```\nexport KAFKA_TOPIC=LIFT_TICKETS_KAFKA_BATCH\npython ./data_generator.py 1 | python ./publish_data.py\n```\n\nA table named LIFT_TICKETS_KAFKA_BATCH should be created in your account."
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "source": "-- There should be 1 row of data which was created by the data_generator. Note: This can take a minute or so to the flush times in configuration.\nUSE ROLE INGEST;\n\nUSE DATABASE INGEST;\nUSE SCHEMA INGEST;\n\nSELECT get_ddl('table', 'LIFT_TICKETS_KAFKA_BATCH');",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7585b1db-b747-46a5-bf4d-1e188a68724e",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- Once we verify that the table was created and it has a single row, we can import all our data\nSELECT count(*) FROM LIFT_TICKETS_KAFKA_BATCH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d13bb32b-7fc9-40ee-a47b-aad11d28c767",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "Run the following in your shell:\n\n```\nexport KAFKA_TOPIC=LIFT_TICKETS_KAFKA_BATCH\ncat data.json.gz | zcat | python ./publish_data.py\n```"
  },
  {
   "cell_type": "code",
   "id": "e4e572f8-2a38-45b9-a83e-28a62a21cd42",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "SELECT count(*) FROM LIFT_TICKETS_KAFKA_BATCH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Tips\n* Every partition will flush to a file when the bytes, time, or records is hit. This can create a LOT of tiny files if not configured well which will be inefficient.\n* Not all workloads can accommodate quick flush times. The more data that is flowing, the quicker data can be visible while being efficient.\n* Reducing the number of partitions and increasing the bytes, time, records to get to well sized files is valuable for efficiency.\n* If you don't have time or a use case to get to well sized files, move to streaming which will match or be better in all cases.\n* Number of tasks, number of nodes in the Kafka Connect cluster, amount of CPU and memory on those nodes, and number of partitions will affect performance and credit consumption.\n* Kafka Connector for Snowflake is billed by the second of compute needed to ingest files (Snowpipe)."
  }
 ]
}