{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2bakv2nbh6joo6dlagwv",
   "authorId": "1011012698186",
   "authorName": "BHARRIS",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "15740345-5cf4-4cd2-96a7-c7c322905f57",
   "lastEditTime": 1738354745334
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## From Kafka - in Snowpipe (Batch) mode\n\nThe table for the data to be written to will be automatically created by the connector.\n\nConfigure and install the connector to load data. Run the following in your shell:\n\n```\nexport KAFKA_TOPIC=LIFT_TICKETS_KAFKA_BATCH\neval $(cat .env)\n\nURL=\"https://$SNOWFLAKE_ACCOUNT.snowflakecomputing.com\"\nNAME=\"LIFT_TICKETS_KAFKA_BATCH\"\n\ncurl -i -X PUT -H \"Content-Type:application/json\" \\\n    \"http://localhost:8083/connectors/$NAME/config\" \\\n    -d '{\n        \"connector.class\":\"com.snowflake.kafka.connector.SnowflakeSinkConnector\",\n        \"errors.log.enable\":\"true\",\n        \"snowflake.database.name\":\"INGEST\",\n        \"snowflake.private.key\":\"'$PRIVATE_KEY'\",\n        \"snowflake.schema.name\":\"INGEST\",\n        \"snowflake.role.name\":\"INGEST\",\n        \"snowflake.url.name\":\"'$URL'\",\n        \"snowflake.user.name\":\"'$SNOWFLAKE_USER'\",\n        \"topics\":\"'$KAFKA_TOPIC'\",\n        \"name\":\"'$NAME'\",\n        \"buffer.size.bytes\":\"250000000\",\n        \"buffer.flush.time\":\"60\",\n        \"buffer.count.records\":\"1000000\",\n        \"snowflake.topic2table.map\":\"'$KAFKA_TOPIC:$NAME'\"\n    }'\n```\n\nVerify the connector was created and is running in the [Redpanda console](http://localhost:8080/topics).\n"
  },
  {
   "cell_type": "markdown",
   "id": "2916bfa4-983f-4d9f-896f-938a8fb6ee26",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "\nTo start, lets push in one message to get the table created and verify the connector is working.\n\nRun the following in your shell:\n\n```\nexport KAFKA_TOPIC=LIFT_TICKETS_KAFKA_BATCH\npython ./data_generator.py 1 | python ./publish_data.py\n```\n\nA table named LIFT_TICKETS_KAFKA_BATCH should be created in your account."
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "source": "-- There should be 1 row of data which was created by the data_generator. Note: This can take a minute or so to the flush times in configuration.\nUSE ROLE INGEST;\n\nUSE DATABASE INGEST;\nUSE SCHEMA INGEST;\n\nSELECT get_ddl('table', 'LIFT_TICKETS_KAFKA_BATCH');",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7585b1db-b747-46a5-bf4d-1e188a68724e",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- Once we verify that the table was created and it has a single row, we can import all our data\nSELECT count(*) FROM LIFT_TICKETS_KAFKA_BATCH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d13bb32b-7fc9-40ee-a47b-aad11d28c767",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "Run the following in your shell:\n\n```\nexport KAFKA_TOPIC=LIFT_TICKETS_KAFKA_BATCH\ncat data.json.gz | zcat | python ./publish_data.py\n```"
  },
  {
   "cell_type": "code",
   "id": "e4e572f8-2a38-45b9-a83e-28a62a21cd42",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "SELECT count(*) FROM LIFT_TICKETS_KAFKA_BATCH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Tips\n* Every partition will flush to a file when the bytes, time, or records is hit. This can create a LOT of tiny files if not configured well which will be inefficient.\n* Not all workloads can accommodate quick flush times. The more data that is flowing, the quicker data can be visible while being efficient.\n* Reducing the number of partitions and increasing the bytes, time, records to get to well sized files is valuable for efficiency.\n* If you don't have time or a use case to get to well sized files, move to streaming which will match or be better in all cases.\n* Number of tasks, number of nodes in the Kafka Connect cluster, amount of CPU and memory on those nodes, and number of partitions will affect performance and credit consumption.\n* Kafka Connector for Snowflake is billed by the second of compute needed to ingest files (Snowpipe)."
  }
 ]
}