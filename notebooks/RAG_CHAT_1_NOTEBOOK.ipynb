{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "26qah5qhtzkkjd6lmcwn",
   "authorId": "394090024763",
   "authorName": "BHARRIS_DEMO",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "732d1511-4c01-4385-94bb-390c838a1ba1",
   "lastEditTime": 1738961811573
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d285729c-a4fe-4d72-a997-b9749feeafa1",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# Build a Retrieval Augmented Generation (RAG) based LLM assistant using Streamlit and Snowflake Cortex Search\n"
  },
  {
   "cell_type": "markdown",
   "id": "22155884-6dee-4baa-9447-ad0fe882459c",
   "metadata": {
    "name": "cell37",
    "collapsed": false
   },
   "source": "Be sure to add langchain to the packages in the top right."
  },
  {
   "cell_type": "markdown",
   "id": "5f9ccc1e-2260-4211-acb6-f7e36854f657",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "### A quick note on chunks\n\nChunk size is the maximum number of characters that a chunk can contain.\nChunk overlap is the number of characters that should overlap between two adjacent chunks.\n\nThe chunk size and chunk overlap parameters can be used to control the granularity of the text splitting. A smaller chunk size will result in more chunks, while a larger chunk size will result in fewer chunks. A larger chunk overlap will result in more chunks sharing common characters, while a smaller chunk overlap will result in fewer chunks sharing common characters.\n\nThere are many different ways to split text. Some common methods include:\nCharacter-based splitting: This method divides the text into chunks based on individual characters.\nWord-based splitting: This method divides the text into chunks based on words.\nSentence-based splitting: This method divides the text into chunks based on sentences.\n\nWe will use the Recursive Text Splitter Module. It is a module in the LangChain library that can be used to split text recursively. This means that the module will try to split the text into different characters until the chunks are small enough.\n\nAn example follows"
  },
  {
   "cell_type": "code",
   "id": "f189c446-dd46-4f32-8ad4-50666d01038e",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "USE DATABASE RAG_CHAT;\nUSE SCHEMA DATA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c155fd62-3a54-4a0b-94c5-876c33381a40",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Import our packages that we'll use later. If you get an error, make sure  your package is added in the 'packages' dropdown above.\n# I have seen scenarios where after adding a package it still says its not available. If that happens, restart your notebook session.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext = \"This is a piece of text.\"\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size = 10, #Adjust this as you see fit\n    chunk_overlap  = 5, #This let's text have some form of overlap. Useful for keeping chunks contextual\n    length_function = len\n)\n\nchunks = splitter.split_text(text)\n\nfor chunk in chunks:\n    print(chunk)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22126130-bcda-458f-a683-48bdd5a59798",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## Organize Documents and Create Pre-Processing Function\n\nCreate a table function that will read the PDF documents and split them in chunks. We will be using the PyPDF2 and Langchain Python libraries to accomplish the necessary document processing tasks. Because as part of Snowpark Python these are available inside the integrated Anaconda repository, there are no manual installs or Python environment and dependency management required.\n"
  },
  {
   "cell_type": "code",
   "id": "50beb326-16d5-4fa7-8d6d-129bb97637af",
   "metadata": {
    "language": "sql",
    "name": "text_chunker_UDTF",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Here we are going to create a function that we will call in Snowflake.\n\ncreate or replace function text_chunker(pdf_text string)\nreturns table (chunk varchar)\nlanguage python\nruntime_version = '3.9'\nhandler = 'text_chunker'\npackages = ('snowflake-snowpark-python', 'langchain')\nas\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport pandas as pd\n\nclass text_chunker:\n\n    def process(self, pdf_text: str):\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 1512, #Adjust this as you see fit, but this is not bad for a pdf document\n            chunk_overlap  = 256, #This will have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(pdf_text)\n        df = pd.DataFrame(chunks, columns=['chunks'])\n        \n        yield from df.itertuples(index=False, name=None)\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6855abfc-0696-42d7-9e6b-ecd6fa57097f",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "If we want to test the function we just created, we can."
  },
  {
   "cell_type": "code",
   "id": "237dc11f-1cd4-43a9-a2ee-e6f5228d08bb",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "select * from table(text_chunker('Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry\\'s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n\nWhy do we use it?\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using \\'Content here, content here\\', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for \\'lorem ipsum\\' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).\n\n\nWhere does it come from?\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\n\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\n\nWhere can I get some?\nThere are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don\\'t look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn\\'t anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary. Use a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.'));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b2b9636-185d-4173-950a-4fffe04d9b24",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Step 2. Download sample [PDF documents](https://github.com/Snowflake-Labs/sfguide-ask-questions-to-your-documents-using-rag-with-snowflake-cortex-search/tree/main)\n\nStep 3. With our function created and tested, we now want to create a Stage with Directory Table where you will be uploading your documents."
  },
  {
   "cell_type": "code",
   "id": "8fca7c02-7e10-49e8-b46f-1091800b9bb7",
   "metadata": {
    "language": "sql",
    "name": "Create_STAGE",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36d19eb0-604d-4c1a-99ec-2fb850465869",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "Step 4. Upload documents to your staging area\n\n- Select Data on the left\n- Click on your database RAG_CHAT\n- Click on your schema DATA\n- Click on Stages and select DOCS\n- On the top right click on the **+Files** botton\n- Drag and drop the PDF documents you downloaded"
  },
  {
   "cell_type": "markdown",
   "id": "ba2f8a6f-3602-4a78-85b3-fda8fba9282c",
   "metadata": {
    "name": "cell38",
    "collapsed": false
   },
   "source": "note to self - files are in the desktop\\DOCAI folder"
  },
  {
   "cell_type": "markdown",
   "id": "090e60b9-7165-46a8-b1dd-6a0ac0a2a88a",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "Step 5. Check files has been successfully uploaded"
  },
  {
   "cell_type": "code",
   "id": "982036d3-7162-4e27-bba7-4f4a13369e22",
   "metadata": {
    "language": "sql",
    "name": "ls_docs",
    "collapsed": false
   },
   "outputs": [],
   "source": "ls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc8317e0-97cb-4bee-8316-23adea17fb68",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## Pre-process and Label Documents\n\nStep 1. Create the table where we are going to store the chunks for each PDF."
  },
  {
   "cell_type": "code",
   "id": "cc188f35-e379-4dff-86f3-54a7355a3147",
   "metadata": {
    "language": "sql",
    "name": "create_table",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create or replace TABLE DOCS_CHUNKS_TABLE ( \n    RELATIVE_PATH VARCHAR(16777216), -- Relative path to the PDF file\n    SIZE NUMBER(38,0), -- Size of the PDF\n    FILE_URL VARCHAR(16777216), -- URL for the PDF\n    SCOPED_FILE_URL VARCHAR(16777216), -- Scoped url (you can choose which one to keep depending on your use case)\n    CHUNK VARCHAR(16777216), -- Piece of text\n    CATEGORY VARCHAR(16777216) -- Will hold the document category to enable filtering\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d2a50d6-4e45-474d-9761-7bb8d5055485",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "Step 2. Use the CORTREX PARSE_DOCUMENT function in order to read the PDF documents from the staging area. Use the function previously created to split the text into chunks. There is no need to create embeddings as that will be managed automatically by Cortex Search service later."
  },
  {
   "cell_type": "code",
   "id": "8102ebb4-08d4-4e27-a134-90c4d1d99aab",
   "metadata": {
    "language": "sql",
    "name": "insert_text_in_table",
    "collapsed": false
   },
   "outputs": [],
   "source": " insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk)\n\n    select relative_path, \n            size,\n            file_url, \n            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n            func.chunk as chunk\n    from \n        directory(@docs),\n        TABLE(text_chunker (TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs, relative_path, {'mode': 'LAYOUT'})))) as func;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eaea2593-4954-4d2b-9e8d-1901637d29c0",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "We can take a look at what this created. In this case, it's nothing too spectacular, just the text from the pdf chunked up (as expected)"
  },
  {
   "cell_type": "code",
   "id": "075c93d6-2dd6-428c-a2e3-7c791a925e1c",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "select * from docs_chunks_table;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17c6b123-c948-413a-a166-632ab93ca60b",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "### Label the product category\n\nWe are going to use the power of Large Language Models to easily classify the documents we are ingesting in our RAG application. We are just going to use the file name but you could also use some of the content of the doc itself. Depending on your use case you may want to use different approaches. We are going to use a foundation LLM but you could even fine-tune your own LLM for your use case.\n\nFirst we will create a temporary table with each unique file name and we will be passing that file name to one LLM using Cortex Complete function with a prompt to classify what that use guide refres too. The prompt will be as simple as this but you can try to customize it depending on your use case and documents. Classification is not mandatory for Cortex Search but we want to use it here to also demo hybrid search.\n\nThis will be the prompt where we are adding the file name `Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' || relative_path || '</file>'`"
  },
  {
   "cell_type": "markdown",
   "id": "e04668d3-ace4-4fa7-8ede-847c57f8c6fb",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "1fc6189b-88fa-445a-aaff-c8c958b94c3e",
   "metadata": {
    "language": "sql",
    "name": "find_categories",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE\nOR REPLACE TEMPORARY TABLE docs_categories AS WITH unique_documents AS (\n  SELECT\n    DISTINCT relative_path\n  FROM\n    docs_chunks_table\n),\ndocs_category_cte AS (\n  SELECT\n    relative_path,\n    TRIM(snowflake.cortex.COMPLETE (\n      'llama3-70b', -- You could use whatever LLM you want here. \n      'Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' || relative_path || '</file>'\n    ), '\\n') AS category\n  FROM\n    unique_documents\n)\nSELECT\n  *\nFROM\n  docs_category_cte;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b3762b4-6338-4f22-aba2-6f29c494694b",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "You can check that table to identify how many categories have been created and if they are correct:"
  },
  {
   "cell_type": "code",
   "id": "203ffad6-1923-4418-8de2-043e7f2ede2e",
   "metadata": {
    "language": "sql",
    "name": "cell19",
    "collapsed": false
   },
   "outputs": [],
   "source": "select category from docs_categories group by category;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6fbfe778-a7d6-4224-b10f-8d9452d0d8b5",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "We can also check that each document category is correct:"
  },
  {
   "cell_type": "code",
   "id": "a9c2f9c4-28fc-40f1-b955-fe4bd8ab39fd",
   "metadata": {
    "language": "sql",
    "name": "cell21",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from docs_categories;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2cf54499-bc22-4524-a834-f676d0518181",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "Now we can just update the table with the chunks of text that will be used by Cortex Search service to include the category for each document:"
  },
  {
   "cell_type": "code",
   "id": "676bfda9-7fcd-41b0-99ae-6f6852c8651b",
   "metadata": {
    "language": "sql",
    "name": "update_categories",
    "collapsed": false
   },
   "outputs": [],
   "source": "update docs_chunks_table \n  SET category = docs_categories.category\n  from docs_categories\n  where  docs_chunks_table.relative_path = docs_categories.relative_path;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5543053e-8f35-4d11-9f70-0dc5e57b1d6c",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "And lets go ahead and see how our table now looks"
  },
  {
   "cell_type": "code",
   "id": "6d6dde41-df8b-4d21-89e1-42d5da232a9f",
   "metadata": {
    "language": "sql",
    "name": "cell23"
   },
   "outputs": [],
   "source": "select * from docs_chunks_table;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a184c76-1b33-435c-8ecf-a7afff307422",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "## Create Cortex Search Service\n\nNext step is to create the CORTEX SEARCH SERVICE in the table we created before.\n\n- The name of the service is CC_SEARCH_SERVICE_CS.\n- The service will use the column chunk to create embeddings and perform retrieval based on similarity search.\n- The column category could be used as a filter.\n- To keep this service updated, warehouse CC_FINS_WH will be used\n- The service will be refreshed every minute.\n- The data retrieved will contain the chunk, relative_path, file_url and category."
  },
  {
   "cell_type": "markdown",
   "id": "d0b53a4d-77d7-4b54-bddd-4f3ac9f9fe32",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "### A few notes about the CORTEX SEARCH SERVICE\n\nWhen you create a Cortex Search Service, Snowflake performs transformations on your source data to get it ready for low-latency serving. When you create a search service, the search index is built as part of the create process. This means the CREATE CORTEX SEARCH SERVICE statement may take longer to complete for larger datasets.\n\n\nThe following command triggers the building of the search service for your data. \n\nIn this example:\n\n- Queries to the service will search for matches in the transcript_text column.\n- The TARGET_LAG parameter dictates that the Cortex Search Service will check for updates to the base table support_transcripts approximately every minute. This would likely be too much in a production system. \n- The columns relative_path, file_url and category will be indexed so that they can be returned along with the results of queries on the transcript_text column.\n- The column category will be available as a filter column when querying the transcript_text column.\n- The warehouse compute_wh will be used for materializing the results of the specified query initially and each time the base table is changed.\n\nNote\n- Depending on the size of the warehouse specified in the query and the number of rows in your table, this CREATE command may take up to several hours to complete.\n- Snowflake recommends using a dedicated warehouse of size no larger than MEDIUM for each service.\n- Columns in the ATTRIBUTES field must be included in the source query, either via explicit enumeration or wildcard, ( * ) .\n"
  },
  {
   "cell_type": "code",
   "id": "6971c036-f7b6-4ab3-935e-39095b44849d",
   "metadata": {
    "language": "sql",
    "name": "create_cortex_search_service",
    "collapsed": false
   },
   "outputs": [],
   "source": "create or replace CORTEX SEARCH SERVICE CC_SEARCH_SERVICE_CS\nON chunk\nATTRIBUTES category\nwarehouse = CC_FINS_WH\nTARGET_LAG = '1 minute'\nas (\n    select chunk,\n        relative_path,\n        file_url,\n        category\n    from docs_chunks_table\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f26d4855-5889-443e-a8e7-850f152775d4",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "At this point, our basic setup is complete and we're now ready to build our chat interface in Streamlit. We can do a quick check to see what's going on with our Search Service though, just to make sure everything is fine\n"
  },
  {
   "cell_type": "code",
   "id": "6c99e9d2-562e-48e5-8c3c-c648ed0dfe05",
   "metadata": {
    "language": "sql",
    "name": "cell28"
   },
   "outputs": [],
   "source": "SELECT PARSE_JSON(\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n      'RAG_CHAT.DATA.CC_SEARCH_SERVICE_CS',\n      '{\n        \"query\": \"Is there a temperature where we should not use our premium bicycle?\",\n        \"columns\":[\n            \"chunk\",\n            \"category\"\n        ],\n        \"filter\": {\"@eq\": {\"category\": \"Bike\"} },\n        \"limit\":1\n      }'\n  )\n)['results'] as results;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68075357-13dd-4e40-99dc-a675a6a48317",
   "metadata": {
    "name": "cell29",
    "collapsed": false
   },
   "source": "Here we can see that it found some relvant content for us (hopefully), but it's not formatted or easy to use. This is simply to show that the search service is working"
  },
  {
   "cell_type": "markdown",
   "id": "fbc96513-75be-4744-9362-586bb133ac8b",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "### At this point, to keep things simple, you can stop and go to the Streamlit Application.\n\nHowever. If you want to go ahead and create a task that will automatically ingest new files, continue on. "
  },
  {
   "cell_type": "markdown",
   "id": "194cb803-58c8-4598-81f9-f7865fa568cc",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "We're going to create a Stream first. This will keep track of new files in our stage."
  },
  {
   "cell_type": "code",
   "id": "dfd26324-835d-4eb5-8a20-9b36df79eca1",
   "metadata": {
    "language": "sql",
    "name": "cell33"
   },
   "outputs": [],
   "source": "CREATE STREAM docs_stream ON STAGE docs;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7492f43-1fe8-4b8d-9286-3300b52aecad",
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "source": "So, we're going to manually refresh the stage here. There are ways to automate this, but just keep in mind that when we add files, refresh the stage so the stream picks them up."
  },
  {
   "cell_type": "code",
   "id": "41c177b4-4276-4a19-8ccc-6944584a7245",
   "metadata": {
    "language": "sql",
    "name": "cell34"
   },
   "outputs": [],
   "source": "ALTER STAGE docs REFRESH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f94743ae-a20c-49b6-b628-cf2e877fc992",
   "metadata": {
    "name": "cell31",
    "collapsed": false
   },
   "source": "Next, we are creating a Snowflake Task. That Task will have some conditions to be executed and one action to take:\n\nWhere: This is going to be executed using warehouse COMPUTE_WH. Please name to your own Warehouse.\n\nWhen: Check every 10 minutes, and execute in the case of new records in the docs_stream stream\n\nWhat to do: Process the files and insert the records in the docs_chunks_table"
  },
  {
   "cell_type": "code",
   "id": "db5aa70f-cb2d-4467-bb40-39cb3a4377e2",
   "metadata": {
    "language": "sql",
    "name": "cell27"
   },
   "outputs": [],
   "source": "create or replace task parse_and_insert_pdf_task \n    warehouse = RAG_CHAT_WH\n    schedule = '10 minute'\n    when system$stream_has_data('docs_stream')\n    as\n  \n    insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk)\n    select relative_path, \n            size,\n            file_url, \n            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n            func.chunk as chunk\n    from \n        docs_stream,\n        TABLE(text_chunker (TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs, relative_path, {'mode': 'LAYOUT'})))) as func;\n\nalter task parse_and_insert_pdf_task resume;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d60a485b-7bfc-4de7-a379-33e593c638b4",
   "metadata": {
    "language": "sql",
    "name": "cell36"
   },
   "outputs": [],
   "source": "-- and if we want to shut down the task\nalter task parse_and_insert_pdf_task suspend;",
   "execution_count": null
  }
 ]
}