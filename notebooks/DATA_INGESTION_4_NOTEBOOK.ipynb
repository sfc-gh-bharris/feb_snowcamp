{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "objak6br6q6kdzdqifay",
   "authorId": "1011012698186",
   "authorName": "BHARRIS",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "06ae9a49-4dc9-479f-ae1a-daf32ea54000",
   "lastEditTime": 1738346446353
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## File Upload & Copy (Snowpipe) using Python\n\nAnother way to get data into Snowflake is to use a service specifically designed for this task: Snowpipe. Snowpipe uses serverless infrastructure to ingest data from a file uploaded from a client. In this use case I will upload a file to an internal stage and call the Snowpipe service to ingest the file.\n\nThis is not the only way to use Snowpipe. You can use external stages as well as use eventing from those blob stores so Snowflake will automatically ingest files as they land. Kafka also uses Snowpipe internally which you will see in later examples.\n\nCreate the table and the snowpipe to handle the ingest. If you changed the data generator for your use case, you will need to change this table to support your data.\n"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "source": "USE ROLE INGEST;\n\nUSE DATABASE INGEST;\nUSE SCHEMA INGEST;\n\nCREATE OR REPLACE TABLE LIFT_TICKETS_PY_SNOWPIPE (TXID varchar(255), RFID varchar(255), RESORT varchar(255), PURCHASE_TIME datetime, EXPIRATION_TIME date, DAYS number, NAME varchar(255), ADDRESS variant, PHONE varchar(255), EMAIL varchar(255), EMERGENCY_CONTACT variant);\n\nCREATE PIPE LIFT_TICKETS_PIPE AS COPY INTO LIFT_TICKETS_PY_SNOWPIPE\nFILE_FORMAT=(TYPE='PARQUET') \nMATCH_BY_COLUMN_NAME=CASE_SENSITIVE;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3515a3bb-5ee3-423d-9cc2-a2e1e2b827e2",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "Take a look at py_snowpipe.py.\n\nThis code will read a batch of lines from standard input, write a file to temporary storage, upload/put that file to LIFT_TICKETS_PY_SNOWPIPE stage, and call the API endpoint to have LIFT_TICKETS_PIPE ingest the file uploaded. Snowpipe will do the COPY INTO the table LIFT_TICKETS_PY_SNOWPIPE.\n\nSince this pattern is creating a file, uploading the file, and copying the results of that data it can VERY efficiently load large numbers of records. It is also only charging for the number of seconds of compute used by Snowpipe.\n\nIn order to test this insert, run the following in your shell:\n\n```\npython ./data_generator.py 1 | python py_snowpipe.py 1\n```"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "source": "-- Query the table to verify the data was inserted. You will probably see 0 records for up to a minute while Snowpipe ingests the file.\n\nSELECT count(*) FROM LIFT_TICKETS_PY_SNOWPIPE;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eb082fb5-1a6d-4147-813e-c0497c805704",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "To send in all your test data, you can run the following in your shell:\n\n```\ncat data.json.gz | zcat | python py_snowpipe.py 10000\n```\n\nThis last call will batch together 10,000 records into each file for processing. As this file gets larger, up to 100mb, you will see this be more efficient on seconds of compute used in Snowpipe and see higher throughputs.\n\nTest this approach with more test data and larger batch sizes. Review INFORMATION_SCHEMA PIPE_USAGE_HISTORY to see how efficient large batches are vs small batches."
  },
  {
   "cell_type": "code",
   "id": "e40ebd85-d334-4fe4-ad42-7b1ee5c23e79",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "-- Query the table to verify the data was inserted. You will probably see 0 records for up to a minute while Snowpipe ingests the file.\n\nSELECT count(*) FROM LIFT_TICKETS_PY_SNOWPIPE;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20d5774b-c0ca-46d4-9f5b-071b044f7f47",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "### Tips\n* Ingest is billed based on seconds of compute used by Snowpipe and number of files ingested.\n* This is one of the most efficient and highest throughput ways to ingest data when batches are well sized.\n* File size is a huge factor for cost efficiency and throughput. If you have files and batches much smaller than 100mb and cannot change them, this pattern should be avoided.\n* Expect delays when Snowpipe has enqueued the request to ingest the data. This process is asynchronous. In most cases these patterns can deliver ~ minute ingest times when including the time to batch, upload, and copy but this varies based on your use case."
  }
 ]
}