{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "46y7l3tdvy2il7n4m75j",
   "authorId": "1011012698186",
   "authorName": "BHARRIS",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "ea096d59-fa35-48da-865e-5b9ac5b10df4",
   "lastEditTime": 1738345422024
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f42566-c534-460c-b85a-f3883033956b",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "## SQL Inserts from the Python Connector\n\nSnowflake has a [Python connector](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector?_fsi=kVaBWUHz&_fsi=kVaBWUHz) which is an easy way to run sql and upload files. One way to get data in would be to do an SQL INSERT statement for each record. While this is a convenient way to insert data, it is not efficient as Snowflake is an OLAP engine and is optimized around writing large batches of data.\n\nCreate a table in Snowflake called LIFT_TICKETS_PY_INSERT to recieve this data from the INGEST user."
  },
  {
   "cell_type": "code",
   "id": "75e8953e-0d81-4792-a7f4-9824105ad352",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "USE ROLE INGEST;\n\nUSE DATABASE INGEST;\nUSE SCHEMA INGEST;\n\n\nCREATE OR REPLACE TABLE LIFT_TICKETS_PY_INSERT (TXID varchar(255), RFID varchar(255), RESORT varchar(255), PURCHASE_TIME datetime, EXPIRATION_TIME date, DAYS number, NAME varchar(255), ADDRESS variant, PHONE varchar(255), EMAIL varchar(255), EMERGENCY_CONTACT variant);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "072c5f14-c2c6-4133-b524-06e1a4266489",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "Now, we're going to go back to VS Code and take a look at py_insert.py\n\nWe will then test a single insert via our shell\n\n```\npython ./data_generator.py 1 | python py_insert.py\n```"
  },
  {
   "cell_type": "code",
   "id": "e044a76a-e8f7-48cd-a1c0-1a9528af1924",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "-- When the above statement is done running, we can check our table.\nSELECT count(*) FROM LIFT_TICKETS_PY_INSERT;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7fb9d279-beb7-44bc-8843-5e0454ba2108",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "Fingers crossed, but you should have one row in that table.\n\n### HOWEVER\n\nThis is not a good way to load data and will take a long time. I don't really want you to have to wait for hours to load your example dataset, so lets just load 1000 records. It is still going to take a very long time.\n\nTo send all your test data, run the following in your shell\n\n```\ncat data.json.gz | zcat | head -n 1000 | python py_insert.py\n```\n"
  },
  {
   "cell_type": "markdown",
   "id": "d02238dd-3976-4cc4-9a96-8a4a0212380e",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "We could parallelize the work if we wanted, so feel free to run this in 10 separate terminals. \n\n```\ncat data.json.gz | zcat | head -n 100 | python py_insert.py\n```\n\nSpoiler alert - this will not help. This is not a good pattern to get a high throughput of records.\n"
  },
  {
   "cell_type": "markdown",
   "id": "def53ba0-1312-4707-bdae-21833aea45eb",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "### Tips\n* Ingest is billed based on warehouse credits consumed while online.\n* The connectors support multi-inserts but data containing a variant field cannot be formatted into a multi-insert.\n* Using inserts and multi-inserts will not efficiently use warehouse resources (optimal at 100MB or more with some concurrency). It is better to upload data and COPY into the table.\n* Connectors will switch to creating and uploading a file and doing a COPY into when large batches are set. This is not configurable.\n* Many assume adding significant concurrency will support higher throughputs of data. The additional concurrent INSERTS will be blocked by other INSERTS, more frequently when small payloads are inserted. You need to move to bigger batches to get more througput.\n* Review query history to see what the connector is doing.\n\nIn cases where the connector has enough data in the executemany to create a well sized file for COPY and does so, this does become as efficient as the following methods.\n\nThe example above could not use executemany as it had VARIANT data.\n\n### The next methods will show how to batch into better sized blocks of work which will drive higher throughputs and higher efficiency on Snowflake.\n"
  }
 ]
}