{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "v5v6irbo5chw3bpdvyvx",
   "authorId": "1011012698186",
   "authorName": "BHARRIS",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "9810b10f-543c-495b-8f56-534cbf37213f",
   "lastEditTime": 1738354112234
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "The following 2 ingest patterns will need Kafka. I will use [Redpanda](https://www.redpanda.com/) in this example, but you could also use Apache or Confluent Kafka as well as MSK from AWS and Event Hubs from Azure.\n\nTo start Kafka locally, we will start with our \"Snowflake Data Ingestion\\docker\\docker-compose.yml\" file\n\nWe will also use our Dockerfile in the same directory\n\nIn your shell, navigate to your directory and run the following\n\n```\ndocker compose up -d\n```\n\nAfter starting up, you will now have a local Kafka Broker at 127.0.0.1:19092 and the Redpanda Console at http://localhost:8080/."
  },
  {
   "cell_type": "markdown",
   "id": "68c73b91-e497-470b-8f4d-6fc9773018a2",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "We will want to add the broker information to our .env file in our /python folder.\n\n```\nREDPANDA_BROKERS=127.0.0.1:19092\n```\n\nYour .env file should look something like this:\n\n```\nLOG_LEVEL=DEBUG\nSNOWFLAKE_ACCOUNT=COB86864\nSNOWFLAKE_USER=INGEST\nPRIVATE_KEY=<KEY HERE>\nREDPANDA_BROKERS=127.0.0.1:19092\n```"
  },
  {
   "cell_type": "markdown",
   "id": "48208c0e-9762-4c5e-bb9c-0c7f22460fce",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "We will not take a look at the publish_data.py file in the python folder.\n\nIt is a Python publisher to take data from standard input and write into the Kafka topic.\n\nTo test the code, you can run the following in your shell:\n```\nexport KAFKA_TOPIC=TESTING\npython ./data_generator.py 1 | python ./publish_data.py\n```\n\nThis should succeed by creating the topic and inserting the data. You can view the success in the [Redpanda console.](http://localhost:8080/topics)"
  }
 ]
}