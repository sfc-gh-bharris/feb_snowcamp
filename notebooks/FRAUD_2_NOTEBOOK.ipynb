{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "vzddu4qaospef5haoamz",
   "authorId": "1011012698186",
   "authorName": "BHARRIS",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "aeb392e0-27f4-42b7-9cf7-aacece8e3cc2",
   "lastEditTime": 1738704745409
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "### Snowflake Feature Store\n\nThe Snowflake Feature Store lets data scientists and ML engineers create, maintain, and use ML features in data science and ML workloads, all within Snowflake.\n\nGenerically, features are data elements used as inputs to a machine learning model. Many columns in a dataset, such as temperature or attendance, can be used as features as-is. In other cases, a column can be made more useful for training via preprocessing and transformation. For example, you might derive a day-of-week feature from a timestamp to allow the model to detect weekly patterns. Other common feature transformations involve aggregating, differentiating, or time-shifting data. Feature engineering is the process of deciding what features are needed by your models and defining how they will be derived from the raw data.\n\nA feature store lets you standardize commonly used feature transformations in a central repository, enabling reuse, helping to reduce duplication of data and effort, and improving productivity. It also helps maintain features by updating them on new source data, always providing correct, consistent, and fresh features in a single source of truth. By cultivating consistency in how features are extracted from raw data, a feature store can also help to make your production ML pipelines more robust.\n\nThe Snowflake Feature Store is designed to make creating, storing, and managing features for data science and machine learning workloads easier and more efficient. Hosted natively inside Snowflake, the Snowflake Feature Store provides the following advantages:\n\n* Your data remains secure, completely under your control and governance, and never leaves Snowflake.\n* The Snowsight Feature Store UI makes it easy to search for and discover features.\n* Access is managed with fine-grained role-based access control.\n\nKey benefits of the Snowflake Feature Store include support for:\n\n* Both batch and streaming data, with efficient automatic updates as new data arrives\n* Backfill and point-in-time correct features with ASOF JOIN\n* Feature transformations authored in Python or SQL\n* Automatic update and refresh of feature values from source data with Snowflake managed Feature Views\n* Ability to use user-managed feature pipelines with external tools such as dbt\n\nThe Snowflake Feature Store is fully integrated with the Snowflake Model Registry and other Snowflake ML features for end-to-end production ML."
  },
  {
   "cell_type": "code",
   "id": "d17ccc67-1ce8-4626-b336-99102a8e45f0",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# Lets just set up a few things\n# Make sure you add the following to the packages\n# altair\n# matplotlib\n# numpy\n# seaborn\n# snowflake-ml-python\n\n# Standard library imports\nimport os\nimport time\nimport math\n\n# Third-party library imports\nimport pandas as pd\nimport numpy as np\n\n\n\n# Snowflake library imports\nimport streamlit as st\n\nimport altair as alt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom snowflake.ml.feature_store import (\nFeatureStore,\nFeatureView,\nCreationMode)\n\nfrom snowflake.ml import dataset\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nsession.query_tag = {\"origin\":\"sf_sit-is\", \n                     \"name\":\"credit_card_fraud\", \n                     \"version\":{\"major\":1, \"minor\":0},\n                     \"attributes\":{\"is_quickstart\":0, \"source\":\"notebook\"}}     \n\n# Set the style for the plots\nsns.set(style=\"whitegrid\")\n\n# Custom color palettes\ncolors = {\n    'Non-Fraud Bars': '#4C72B0',\n    'Fraud Bars': '#55A868',\n    'Non-Fraud Line': '#1f77b4',\n    'Fraud Line': '#ff7f0e'\n}",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1644b14b-4e44-4954-91e2-9f9d206c0e78",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "use warehouse CC_FINS_WH;\nuse database CC_FINS_DB;\nuse schema analytics;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d00bf567-410d-4631-a404-5ab9da54cf7d",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "### Generating Datasets for Training\n \nWe are now ready to generate our training set. We'll define a spine DataFrame to form the backbone of our generated dataset and pass it into FeatureStore.generate_dataset() along with our Feature Views.\n\nNOTE: The spine serves as a request template and specifies the entities, labels and timestamps (when applicable). The feature store then attaches feature values along the spine using an AS-OF join to efficiently combine and serve the relevant, point-in-time correct feature data."
  },
  {
   "cell_type": "code",
   "id": "bf2d85bc-5b26-4f98-ac82-3e6357909344",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "create or replace TABLE TRANSACTIONS_DATA (USER_ID VARCHAR,TRANSACTION_ID VARCHAR(16777216),IS_FRAUD VARCHAR);\n\ninsert into TRANSACTIONS_DATA(User_ID, Transaction_ID, IS_FRAUD) SELECT distinct User_ID, Transaction_ID, IS_FRAUD FROM CREDITCARD_TRANSACTIONS;\n\nselect * from CREDITCARD_TRANSACTIONS limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84c28faa-d584-4e7b-b556-bf205cf3c178",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "full_df = session.sql(\"SELECT * FROM CREDITCARD_TRANSACTIONS\")\ndataset=full_df.toPandas()\n\nTRANSACTIONS_DATA_df = session.table(\"TRANSACTIONS_DATA\")\n\n\ndf= TRANSACTIONS_DATA_df.select( F.col(\"TRANSACTION_ID\"),F.col(\"IS_FRAUD\")).groupBy(F.col(\"IS_FRAUD\")) \\\n          .agg(F.count_distinct(F.col(\"TRANSACTION_ID\")).alias(\"TOTAL_FRAUD\")) \n\n# Visualization of the fraud and normal data using a bar chart displayed in Streamlit. Shows the total number of distinct transactions for each fraud category.\nst.bar_chart(df,x=\"IS_FRAUD\",y=\"TOTAL_FRAUD\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dc093cb3-993d-4994-9dc6-497a917101e4",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# Create a histogram that shows the distribution of transaction amounts, distinguishing between fraudulent and non-fraudulent transactions. \n\n \ndataset['IS_FRAUD'] = dataset['IS_FRAUD'].astype(int)\n# Set the style for the plots\nsns.set(style=\"whitegrid\")\n# Background color\nbackground_color = \"#f0f0f0\"  # Light gray\n# 1. Distribution of Transaction Amounts\nplt.figure(figsize=(4,4))\nsns.histplot(data=dataset, x='TRANSACTION_AMOUNT', hue='IS_FRAUD', kde=True, bins=50)\nplt.title('Distribution of Transaction Amounts')\nplt.xlabel('Transaction Amount')\nplt.ylabel('Frequency')\nplt.legend(title='Transaction', loc='upper right', labels=['Normal', 'Fraud'])\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# Create a histogram that shows the distribution of clicks, distinguishing between fraudulent and non-fraudulent transactions. \n#CLICKS, LOGIN_PER_HOUR, and PAGES_VISITED Distributions\n\nsns.set(style=\"whitegrid\")\n\n# Custom color palettes\ncolors = {\n    'Normal Bars': '#4C72B0',\n    'Fraud Bars': '#55A868',\n    'Normal Line': '#1f77b4',\n    'Fraud Line': '#ff7f0e'\n}\n# 4. CLICKS Distribution\nplt.figure(figsize=(4, 4))\nsns.histplot(data=dataset, x='CLICKS', hue='IS_FRAUD', multiple='dodge', kde=True, bins=30)\nplt.title('Clicks Distribution')\nplt.xlabel('Clicks')\nplt.ylabel('Frequency')\nplt.legend(title='Transaction', loc='upper right', labels=['Normal', 'Fraud'])\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b7d867ab-8d4d-4ad5-bc7d-3646fa20b554",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Create a histogram that shows the distribution of logins, distinguishing between fraudulent and non-fraudulent transactions.  \n\nplt.figure(figsize=(4, 4))\nsns.histplot(data=dataset, x='LOGIN_PER_HOUR', hue='IS_FRAUD', multiple='dodge', kde=True, bins=30)\nplt.title('Login Per Hour Distribution')\nplt.xlabel('Login Per Hour')\nplt.ylabel('Frequency')\nplt.legend(title='Is Fraud', loc='upper right', labels=['Non-Fraud', 'Fraud'])\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a50826f5-5ae0-4c7b-a524-d0874d20417d",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "# Create a histogram that shows the distribution of time elapsed online, distinguishing between fraudulent and non-fraudulent transactions.  \n\nplt.figure(figsize=(4,4))\n\nsns.histplot(data=dataset, x='TIME_ELAPSED', hue='IS_FRAUD', kde=True, bins=50)\nplt.title('Time Elapsed Distribution')\nplt.xlabel('Time Elapsed (seconds)')\nplt.ylabel('Frequency')\nplt.legend(title='Is Fraud', loc='upper right', labels=['Non-Fraud', 'Fraud'])\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71f10e23-8484-4f91-9ffc-3abf10fa2280",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "# Create a histogram that shows the distribution of location, distinguishing between fraudulent and non-fraudulent transactions.  \n# Define location coordinates\nlocation_coords = {\n    'New York': (40.7128, -74.0060),\n    'Los Angeles': (34.0522, -118.2437),\n    'Chicago': (41.8781, -87.6298),\n    'Houston': (29.7604, -95.3698),\n    'Phoenix': (33.4484, -112.0740),\n    'Philadelphia': (39.9526, -75.1652),\n    'San Antonio': (29.4241, -98.4936),\n    'San Diego': (32.7157, -117.1611),\n    'Dallas': (32.7767, -96.7970),\n    'San Jose': (37.3382, -121.8863),\n    'Moscow': (55.7558, 37.6176)  # Add Moscow coordinates\n}\n\n# Add latitude and longitude based on location\ndataset['LATITUDE'] = dataset['LOCATION'].map(lambda loc: location_coords.get(loc, (None, None))[0])\ndataset['LONGITUDE'] = dataset['LOCATION'].map(lambda loc: location_coords.get(loc, (None, None))[1])\n\n# Filter for plotting\nplt.figure(figsize=(6, 6))\n\n# Plot all locations\nscatter = plt.scatter(dataset['LONGITUDE'], dataset['LATITUDE'], \n                      c=dataset['IS_FRAUD'].map({0: 'purple', 1: 'red'}),\n                      alpha=0.5)\n\n# Create custom legend\npurple_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='purple', markersize=10, label='Normal')\nred_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Fraud')\n\n# Plot details\nplt.title('Geographical Distribution of Transactions')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\n# Set legend with custom handles\nplt.legend(handles=[purple_patch, red_patch], title='Transaction Type', loc='upper left', bbox_to_anchor=(1, 1), frameon=True, fontsize='small')\n\nplt.grid(True)\n\n# Set background color for the plot\nplt.gcf().set_facecolor(\"#f0f0f0\")  # Light gray\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af73f7a4-cc78-474d-86ee-49462c765caa",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "### Feature Store\n\nThe feature store contains feature views for customers and transactions. Model features will be accessed from the feature store."
  },
  {
   "cell_type": "code",
   "id": "5e685f54-4064-40bf-9d78-262646fa83c3",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "# Access feature views\n\nfs = FeatureStore(\n    session=session,\n    database=\"CC_FINS_DB\",\n    name=\"ANALYTICS\",\n    default_warehouse=\"CC_FINS_WH\",\n    creation_mode=CreationMode.FAIL_IF_NOT_EXIST\n)\n\ncustomer_fv : FeatureView = fs.get_feature_view(\n    name='Customer_Features',\n    version='V1'\n)\nprint(customer_fv)\n\ntrans_fv : FeatureView = fs.get_feature_view(\n    name='Trans_Features',\n    version='V1'\n)\nprint(trans_fv)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0c7fe01-1a88-4c61-b4cf-46bcee6a549b",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "We can go over to Snowsight and we should be able to see the views that were created (CC_FINS_DB.ANALYTICS.CUSTOMER_FEATURES$V1) as well as by going to the AI&ML option on the left menu and then the Features option.\n"
  },
  {
   "cell_type": "code",
   "id": "27fefcf1-7195-47ef-ae56-dee7097044a0",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "# Generate a training data set with the feature store’s generate_training_set method, which enriches a Snowpark DataFrame that contains the source data with the derived feature values.  \n# Get transactions dataset and get features from the feature store\ndef create_dataset(spine_df, name):\n    train_dataset = fs.generate_dataset(\n    name=name,\n    spine_df=spine_df,\n    features=[customer_fv, trans_fv]\n    )\n    df = train_dataset.read.to_snowpark_dataframe()\n    return df\n# Split into train/validation/test\n\ndatasets = TRANSACTIONS_DATA_df.random_split([.8,.2])\n\n# Build training tables\ntrain_df = create_dataset(datasets[0], \"train\")\nval_df = create_dataset(datasets[1], \"validation\")\n\n#View the training dataset.This contains the columns except for Ids. The Label is included here as this will be specified in the LABEL field during model training. \ntrain_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13b0b3e7-ee63-4b7c-ae7f-420b7fe32520",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "# Create separate views for training and validation to be used with a Binary Classifier. \n# Columns in the inference data that were not present in the training dataset are ignored. \n\ntrain_df.write.mode(\"overwrite\").save_as_table(\"training_fd_table\")\n\nsession.sql(\"CREATE OR REPLACE VIEW fraud_classification_training_view AS SELECT IS_FRAUD,LATITUDE,LONGITUDE,LOCATION,TOTAL_TRANSACTIONS,STDDEV_TRANSACTION_AMOUNT,NUM_UNIQUE_MERCHANTS, MEAN_WEEKLY_SPENT,MEAN_MONTHLY_SPENT,MEAN_YEARLY_SPENT,TIME_ELAPSED,CLICKS,CUMULATIVE_CLICKS,CUMULATIVE_LOGINS_PER_HOUR FROM training_fd_table\").collect()\n\nval_df.drop(\"IS_FRAUD\").collect()\nval_df.write.mode(\"overwrite\").save_as_table(\"val_fd_table\")\n\nsession.sql(\"CREATE OR REPLACE VIEW fraud_classification_val_view AS SELECT * EXCLUDE IS_FRAUD FROM val_fd_table\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e869d517-ba88-4cff-90b5-c6406204e346",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": "SELECT * FROM fraud_classification_val_view LIMIT 2;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12ca3e35-e6af-4cf1-8488-bf99b41d7d4f",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "### Build the model\nWe can create the classification model by running the following statement "
  },
  {
   "cell_type": "code",
   "id": "8791abae-a5e4-4b20-9269-488d8fac9d81",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE SNOWFLAKE.ML.CLASSIFICATION fraud_classification_model(\n    INPUT_DATA => SYSTEM$REFERENCE('view', 'fraud_classification_training_view'),\n    TARGET_COLNAME => 'IS_FRAUD'\n);\n\n-- View all classification models, use the SHOW command. \nSHOW SNOWFLAKE.ML.CLASSIFICATION;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16a85bba-48d7-408a-9d4a-51f12b1b4311",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": "-- Add a table to use for the Streamlit App that will be used for ongoing Predictions \n\nCREATE or replace table CC_APP_TBL AS SELECT * FROM CREDITCARD_TRANSACTIONS WHERE TRANSACTION_ID NOT IN (SELECT DISTINCT TRANSACTION_ID FROM training_fd_table);\nalter table CC_APP_TBL drop column IS_FRAUD;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "067705f8-bb77-49a9-a3f9-296d433a5eb9",
   "metadata": {
    "language": "sql",
    "name": "cell21"
   },
   "outputs": [],
   "source": "-- Run inference (prediction) on a dataset, use the model’s PREDICT method. \n\nCREATE OR REPLACE TABLE fraud_predictions AS\nSELECT *,fraud_classification_model!PREDICT(INPUT_DATA => object_construct(*)) as predictions\nfrom fraud_classification_val_view;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4778dfae-5774-4c78-97ac-28ca2c368290",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "### View the predictions.\n\nThe prediction object includes predicted probabilities for each class and the predicted class based on the maximum predicted probability. \n\nThe predictions are returned in the same order as the original features were provided."
  },
  {
   "cell_type": "code",
   "id": "6fbecb22-4cd8-4fe0-9b1e-fe44e8ff6121",
   "metadata": {
    "language": "sql",
    "name": "cell23"
   },
   "outputs": [],
   "source": " SELECT * FROM fraud_predictions;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9284fd83-c1ee-4c0c-92fe-1abfdbd16215",
   "metadata": {
    "language": "sql",
    "name": "cell24"
   },
   "outputs": [],
   "source": "-- In the result set, we see that the model produces both a predicted class denoted by class as well giving us the probability of the respective class membership. \n-- Oftentimes, we may want to parse out the probabilities or the prediction directly, and have it in its own column \n\nselect * EXCLUDE PREDICTIONS,\n        predictions:class::STRING AS class,\n        predictions['probability'][class] as probability\nfrom fraud_predictions;\n\n-- we can see from our data that it's very easy to classify if transactions are fraud",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72e48c60-355e-49f0-b268-0b777c0e6047",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "Now that we have built our classifier, we can begin to evaluate it to better understand both its performance as well as the primary factors within the dataset that were driving the predictions. Follow along below to see the various commands you may run to evalute your own classifier:"
  },
  {
   "cell_type": "markdown",
   "id": "3b44ba28-4631-4b8e-b47f-ed5f585e461a",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "### Confusion Matrix & Model Accuracy\nOne of the most common ways of evaluating a classifier is by creating a Confusion Matrix, which allows us to visualize the types of errors that the model is making. Typically, they are used to calculate a classifier's Precision & Recall; which describe both the accuracy of a model when it predicts a certain class of interest (Precision), as well as how many of that specific class of interest were classified (recall)\n\nReturns a table containing the number of instances of each combination of actual class and predicted class in models where evaluation was enabled at instantiation. You can use this dataset to plot a confusion matrix. This method takes no arguments. See Confusion Matrix in show_confusion_matrix.\n\ndataset_type - The name of the dataset used for metrics calculation, currently EVAL.\n\nactual_class - The actual class.\n\npredicted_class - The predicted class.\n\ncount - The number of instances of the given combination of actual and predicted class.\n\nlogs - Contains error or warning messages."
  },
  {
   "cell_type": "code",
   "id": "6aff9d01-59c1-42fc-8078-fe18b31e4428",
   "metadata": {
    "language": "sql",
    "name": "cell27"
   },
   "outputs": [],
   "source": " CALL fraud_classification_model!SHOW_CONFUSION_MATRIX(); ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "63e0f2bb-c6f9-4a59-8dfe-dac391ac1557",
   "metadata": {
    "name": "cell28",
    "collapsed": false
   },
   "source": "### Evaluation Metrics\n\nThe show_evaluation_metrics calculates the following False Positive, False Negative, True Positive and True Negative\n\nTo get the evaluation metrics for your model, call the <model_name>!SHOW_EVALUATION_METRICS method. By default, the forecasting function evaluates all models it trains using a method called cross-validation. This means that under the hood, in addition to training the final model on all of the training data you provide, the function also trains models on subsets of your training data. Those models are then used to predict your target metric on the withheld data, allowing the function to compare those predictions to actual values in your historical data.\n\nIf you don’t need these evaluation metrics, you can set evaluate to FALSE. If you want to control the way cross-validation is run, you can use the following parameters:"
  },
  {
   "cell_type": "code",
   "id": "b172196a-51c4-4172-9945-bb61027ea298",
   "metadata": {
    "language": "sql",
    "name": "cell29"
   },
   "outputs": [],
   "source": " CALL fraud_classification_model!SHOW_EVALUATION_METRICS();",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a8200b4-49bd-4d58-aaea-3dea38588533",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "### Threshold Metrics\n\nshow_threshold_metrics provides raw counts and metrics for a specific threshold for each class. This can be used to plot ROC and PR curves or do threshold tuning if desired. The threshold varies from 0 to 1 for each specific class; a predicted probability is assigned.\n\nThe sample is classified as belonging to a class if the predicted probability of being in that class exceeds the specified threshold. The true and false positives and negatives are computed considering the negative class as every instance that does not belong to the class being considered. The following metrics are then computed.\n\nTrue positive rate (TPR): The proportion of actual positive instances that the model correctly identifies (equivalent to Recall).\n\nFalse positive rate (FPR): The proportion of actual negative instances that were incorrectly predicted as positive.\n\nAccuracy: The ratio of correct predictions (both true positives and true negatives) to the total number of predictions, an overall measure of how well the model is performing. This metric can be misleading in unbalanced cases.\n\nSupport: The number of actual occurrences of a class in the specified dataset. Higher support values indicate a larger representation of a class in the dataset. Support is not itself a metric of the model but a characteristic of the dataset."
  },
  {
   "cell_type": "code",
   "id": "e0a468a5-23e9-4fa4-9134-94a21926e043",
   "metadata": {
    "language": "sql",
    "name": "cell31"
   },
   "outputs": [],
   "source": "CALL fraud_classification_model!SHOW_THRESHOLD_METRICS()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e236dd3a-f006-4163-bd70-7a71ebfab309",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "### Feature Importances\nThe last thing we want to understand when evaluating the classifier is to get a sense of the importance of each of the individual input columns or features we made use of. \n\nBetter understand what's driving a model's prediction to give us more insight into the business process we are trying to model out\nEngineer new features or remove ones that are not too impactful to increase the model's performance.\nThe ML Classification function provides a method to do just this, and provides us a ranked list of the relative importance of all the input features, such that their values are between 0 and 1, and the importances across all the features sum to be 1."
  },
  {
   "cell_type": "code",
   "id": "12746abe-927b-4310-b9c6-bf216e7ee9d1",
   "metadata": {
    "language": "sql",
    "name": "cell33"
   },
   "outputs": [],
   "source": "CALL fraud_classification_model!SHOW_FEATURE_IMPORTANCE();",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47b12a33-d863-474d-9e10-42cd22154bf1",
   "metadata": {
    "name": "cell34",
    "collapsed": false
   },
   "source": "We're done with this part of the tutorial. Depending on the time available, we will now take a look at a Streamlit application built to support Fraud Detection"
  }
 ]
}