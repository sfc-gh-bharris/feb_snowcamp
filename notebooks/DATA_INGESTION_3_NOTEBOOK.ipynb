{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "tiyj57qhagklesphftnr",
   "authorId": "1011012698186",
   "authorName": "BHARRIS",
   "authorEmail": "bradley.harris@snowflake.com",
   "sessionId": "de7447ed-a439-4352-ae1a-aa8e55da8acd",
   "lastEditTime": 1738346101553
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## File Upload & Copy (Warehouse) from the Python Connector\n\nTo get to better sized batches, the client can upload a file and have a warehouse copy the data into the destination. The Python connector can execute the COPY after uploading the file.\n\nCreate the table which will be used for landing the data, changing as needed for your use case."
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "source": "USE ROLE INGEST;\n\nUSE DATABASE INGEST;\nUSE SCHEMA INGEST;\n\nCREATE OR REPLACE TABLE LIFT_TICKETS_PY_COPY_INTO (TXID varchar(255), RFID varchar(255), RESORT varchar(255), PURCHASE_TIME datetime, EXPIRATION_TIME date, DAYS number, NAME varchar(255), ADDRESS variant, PHONE varchar(255), EMAIL varchar(255), EMERGENCY_CONTACT variant);",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1df76cb7-3f87-40be-a5e4-60b5dca7390d",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "Take a look at the py_copy_into.py\n\nYou will see a lot of similarity of this pattern with the previous one in that the connection is the same, but instead of doing single record inserts it batches together a set of records. That batch is written into a Parquet file which is PUT to the table stage and COPY is used to insert. This data shows up immediately after the COPY call is made.\n\nIn order to test this insert, run the following in your shell:\n\n```\npython ./data_generator.py 1 | python py_copy_into.py 1\n```\n"
  },
  {
   "cell_type": "code",
   "id": "24489dc9-a0d7-4b30-8ccc-a3ffb6a1ce43",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- Query the table to verify the data was inserted.\nSELECT count(*) FROM LIFT_TICKETS_PY_COPY_INTO;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e367278-42b8-4f46-be30-078690cf0cb3",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "To send in all your test data, you can run the following in your shell:\n```\ncat data.json.gz | zcat | python py_copy_into.py 10000\n```\n\nThis last call will batch together 10,000 records into each file for processing. As this file gets larger, up to 100mb, you will see this be more efficient on seconds of compute used in Snowpipe and see higher throughputs. Feel free to generate more test data and increase this to get more understanding of this relationship. Review the query performance in Query History in Snowflake."
  },
  {
   "cell_type": "markdown",
   "id": "98c2e651-3dca-49fb-b707-d2154960d616",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "### Tips\n* Ingest is billed based on warehouse credits consumed while online.\n* It is very hard to fully utilize a warehouse with this pattern. Adding some concurrency will help IF the files are already well sized. Even with the best code, very few workloads have fixed data flow volumes that well match a warehouse. This is mostly a wasted effort as serverless and snowpipe solves all use cases w/o constraints.\n* Try to get to 100mb files for most efficiency.\n* Best warehouses sizes are almost always way smaller than expected, commonly XS."
  }
 ]
}